{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"IBB-Ear_recognition.ipynb","provenance":[{"file_id":"1BBe1aKzUTUBLWP1IVM4Hz9vEONQbUNNI","timestamp":1610311611267}],"collapsed_sections":["Y6v0WNEHehav"],"toc_visible":true,"mount_file_id":"1BBe1aKzUTUBLWP1IVM4Hz9vEONQbUNNI","authorship_tag":"ABX9TyPvLr+24me94E0R1IQybWE3"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6l4FOhVPjPWx"},"source":["# IBB Assignment 3 - Ear recognition\n","Faculty of Computer Science, Univeristy of Ljubljana\n","\n","David Valiƒç - dv6510@student.uni-lj.si\n","\n","\n","# Instructions for running this notebook:\n","\n","\n","\n","**Section 0: Google Drive Setup**\n","\n","In this section load google drive and move to the directory where you have the following files:\n","\n","(in this case, you need to run section 1.3 and 3.2)\n","*   `awe_train.csv`\n","*   `awe_test.csv`\n","*   `awe` folder with all the cropped ear images\n","\n","OR\n","\n","(in this case, you need to run 1.5 and 3.4)\n","*   `train_dataset.pckl`\n","*   `test_dataset.pckl`\n","\n","\n","Optionally you can also have a model which you wish to evaluate on test dataset.\n","\n","\n","\n","**Section 1: CNN Setup**\n","\n","In this section run:\n","\n","*   **1.1** AND \n","*   **1.2** AND\n","*   Either **1.3** if you don't have `train_dataset.pckl` in your dicrectory OR **1.5** if you have `train_dataset.pckl` in your directory AND\n","*  **1.6**\n","\n","\n","\n","**Section 2: CNN Training**\n","\n","In this section run:\n","\n","*   Either : **2.1.a** for *identity* OR **2.1.b** for *ethnicity* OR **2.1.c** for gender classification model training AND\n","*   **2.2** AND\n","*   **2.3**\n","\n","<br>\n","<br>\n","<br>\n","\n","**Sections 3 & 4: Evaluation**\n","*   If you wish to evaluate models from all epochs from a single training go to section **3**. \n","*   If you wish to evaluate a single model go to section **4**.\n","\n","**Section 3: CNN Evaluation**\n","\n","In this section run:\n","\n","*   Either : **3.1.a** for *identity* OR **3.1.b** for *ethnicity* OR **3.1.c** for gender model evaluation AND\n","*   Either **3.2** if you don't have `test_dataset.pckl` in your dicrectory OR **3.4** if you have `test_dataset.pckl` in your directory AND\n","*   **3.6**\n","\n","**Section 4: Signle CNN model Evaluation**\n","\n","In this section run:\n","*   Either **4.1** if you don't have `test_dataset.pckl` in your dicrectory OR **4.2** if you have `test_dataset.pckl` in your directory AND\n","*   Either : **4.3.a** for *identity* OR **4.3.b** for *ethnicity* OR **4.3.c** for gender model evaluation AND\n","*   **4.4** AND\n","*   **4.5**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5T6BMlQV86W0"},"source":["# 0. Google Drive Setup"]},{"cell_type":"markdown","metadata":{"id":"x8a9cIzONsIy"},"source":["**0.1** Mount google drive to the project"]},{"cell_type":"code","metadata":{"id":"6AqTU4lWdtjl"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive/')\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pyqIpsFIN3lV"},"source":["**0.2** Move to the directory of the 3. assignemnt where `awe-train.csv`, `awe-test.csv` and whole `awe` images directory are"]},{"cell_type":"code","metadata":{"id":"IfEDbXSZdp18"},"source":["%cd /content/drive/MyDrive/mg/3. semester/SB/Naloge/naloga3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8pi9JE482oc"},"source":["# 1. CNN Setup"]},{"cell_type":"markdown","metadata":{"id":"R7KS1E_zOHV7"},"source":["**1.1** Create a class for common methods used to reshape data, also se the image width and heigth used throught the whole notebook"]},{"cell_type":"code","metadata":{"id":"aEUlWWslOM4I"},"source":["import numpy\n","\n","IMG_WIDTH=224\n","IMG_HEIGHT=224\n","\n","class Common:\n","\n","    @staticmethod\n","    def reshape_transform_data(data):\n","        data = numpy.array(data)\n","        result = Common.reshape_data(data)\n","        return Common.to_float(result)\n","\n","    @staticmethod\n","    def reshape_data(data):\n","        return data.reshape(data.shape[0], IMG_WIDTH, IMG_HEIGHT, 3)\n","\n","    @staticmethod\n","    def reshape_from_img(image):\n","        return image.reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n","\n","    @staticmethod\n","    def to_float(value):\n","        return value.astype('float32')/255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9BZRIYrO2Je"},"source":["**1.2** Initialize the training data class - the images and the labels for identities, ethnicities and genders"]},{"cell_type":"code","metadata":{"id":"hUHM071ObfXs"},"source":["import abc\r\n","import os\r\n","import random\r\n","from abc import abstractmethod\r\n","\r\n","from keras_preprocessing.image import load_img, img_to_array\r\n","from skimage import io\r\n","from sklearn.model_selection import train_test_split\r\n","from tensorflow.python.keras.utils import np_utils\r\n","import json\r\n","\r\n","FILE_TRAIN_CSV = \"awe-train.csv\"\r\n","\r\n","\r\n","class EarDataSet(metaclass=abc.ABCMeta):\r\n","\r\n","    def __init__(self, classes_identity, classes_ethnicity, classes_gender):\r\n","        # to je steilo oseb - pri AWE je 100\r\n","        self.n_classes = classes_identity\r\n","        self.n_identity_classes = classes_identity\r\n","        self.n_ethnicity_classes = classes_ethnicity\r\n","        self.n_gender_classes = classes_gender\r\n","        # objects so podatki slik, ki so prebrani z io.imread() in potem reshapani ampak bo jih treba resizat tudi, tako da uporabi\r\n","        self.objects = []\r\n","        # labels so train data in so vektorji dolgi 100, ki imajo povsod 0, razen pri osebi ki je ta prava imajo 1\r\n","        self.labels = []\r\n","        self.labels_identity = []\r\n","        self.labels_ethnicity = []\r\n","        self.labels_gender = []\r\n","        # obj_validation so podatki slik, ki so prebrani z io.imread() in potem reshapani ampak bo jih treba resizat tudi, tako da uporabi\r\n","        self.obj_validation = []\r\n","        # labels_validation so test data in so vektorji dolgi 100, ki imajo povsod 0, razen pri osebi ki je ta prava imajo 1\r\n","        self.labels_validation = []\r\n","        # number_labels je steilo vseh slik, ki sodelujejo v treniranju CNN modela\r\n","        self.number_labels = 0\r\n","\r\n","    def get_data(self):\r\n","        self.objects, self.labels_identity, self.labels_ethnicity, self.labels_gender = self.fetch_img_path()\r\n","        self.process_data()\r\n","        #self.print_dataSet()\r\n","\r\n","    def process_data(self):\r\n","        self.labels_identity = np_utils.to_categorical(self.labels_identity, self.n_identity_classes)\r\n","        self.labels_ethnicity = np_utils.to_categorical(self.labels_ethnicity, self.n_ethnicity_classes)\r\n","        self.labels_gender = np_utils.to_categorical(self.labels_gender, self.n_gender_classes)\r\n","        self.objects = Common.to_float(numpy.asarray(self.objects, dtype= numpy.float32))\r\n","\r\n","\r\n","    def fetch_img_path(self):\r\n","        images = []\r\n","        labels_identity = []\r\n","        labels_ethnicity = []\r\n","        labels_gender = []\r\n","        for line in open(FILE_TRAIN_CSV):\r\n","            csv_row = line.split(\",\")\r\n","            file_name = csv_row[1]\r\n","            with open('awe/'+file_name.split('/')[0]+'/annotations.json') as f:\r\n","              data = json.load(f)\r\n","              ethnicity = int(data[\"ethnicity\"])-1\r\n","              if ethnicity is 98:\r\n","                ethnicity = 7\r\n","              labels_ethnicity.append(ethnicity)\r\n","              gender = 1 # 1 means female, 0 means male\r\n","              if data[\"gender\"] is \"m\":\r\n","                gender = 0\r\n","              elif data[\"gender\"] is \"f\":\r\n","                gender = 1\r\n","              else:\r\n","                gender = 2 \r\n","              labels_gender.append(gender)\r\n","            image = load_img(\"awe/\"+file_name, target_size=(IMG_WIDTH,IMG_HEIGHT))\r\n","            image = img_to_array(image)\r\n","            image = Common.reshape_from_img(image)\r\n","            label = int(csv_row[2])-1\r\n","            images.append(image)\r\n","            labels_identity.append(label)\r\n","            self.number_labels +=1\r\n","\r\n","        return images, labels_identity, labels_ethnicity, labels_gender\r\n","\r\n","    def print_dataSet(self):\r\n","        print(self.objects)\r\n","        print(self.labels)\r\n","    \r\n","    def set_identity_labels(self):\r\n","        self.labels = self.labels_identity\r\n","        self.n_classes = self.n_identity_classes\r\n","\r\n","    def set_ethnicity_labels(self):\r\n","        self.labels = self.labels_ethnicity\r\n","        self.n_classes = self.n_ethnicity_classes\r\n","\r\n","    def set_gender_labels(self):\r\n","        self.labels = self.labels_gender\r\n","        self.n_classes = self.n_gender_classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBs-aRwZbZBc"},"source":["**1.3 (or skip to 1.5 if train data file is already present)** load actual training data into dataSet variable"]},{"cell_type":"code","metadata":{"id":"jZb7AUblbYNq"},"source":["n_identity_classes = 100\n","n_ethnicity_classes = 8\n","n_gender_classes = 2\n","\n","dataSet = EarDataSet(n_identity_classes, n_ethnicity_classes, n_gender_classes)\n","\n","dataSet.get_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xH0toCs7Zy0B"},"source":["**1.4 (optional)** Save loaded dataset into a file for easier later loading"]},{"cell_type":"code","metadata":{"id":"iJ6P__c5YLyk"},"source":["import pickle\n","f = open('train_dataset.pckl', 'wb')\n","pickle.dump(dataSet, f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YpeQfKdpZ-r5"},"source":["**1.5 (optional) - (execute 1.3 and 1.4 if no file is present)** Loads saved dataset into a variable"]},{"cell_type":"code","metadata":{"id":"RbP0dVKbYYqD"},"source":["import pickle\n","f = open('train_dataset.pckl', 'rb')\n","dataSet = pickle.load(f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGKuOZ_8PAG6"},"source":["**1.6** Define the CNN model"]},{"cell_type":"code","metadata":{"id":"Bqn2pwqWg-VE"},"source":["import numpy\r\n","from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n","from tensorflow.python.keras.layers import Convolution2D, Activation, MaxPooling2D, Dropout, Flatten, Dense\r\n","from tensorflow.python.keras.models import Sequential\r\n","from tensorflow.python.keras.optimizer_v2.gradient_descent import SGD\r\n","import datetime\r\n","import matplotlib.pyplot as plt\r\n","\r\n","# reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, mode='auto')\r\n","# early = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=1, mode='auto')\r\n","\r\n","\r\n","class ConvolutionalModel():\r\n","\r\n","    def __init__(self, dataSet=None):\r\n","        if dataSet is None:\r\n","            raise Exception(\"DataSet is required in this model\")\r\n","        self.shape = numpy.array([IMG_WIDTH, IMG_HEIGHT, 3])\r\n","        self.model_summary= \"\"\r\n","        if dataSet is not None:\r\n","            self.objects = dataSet.objects\r\n","            self.labels = dataSet.labels\r\n","            self.obj_validation = dataSet.obj_validation\r\n","            self.labels_validation = dataSet.labels_validation\r\n","            self.number_labels = dataSet.number_labels\r\n","            self.n_classes = dataSet.n_classes\r\n","        self.init_model()\r\n","        self.cnn.compile(loss='categorical_crossentropy',\r\n","                         optimizer=SGD(lr=LEARNING_RATE, decay=1e-65, momentum=0.92, nesterov=True),\r\n","                         metrics=['accuracy'])\r\n","\r\n","    def init_model(self, simple = False):\r\n","        self.cnn = Sequential()\r\n","        if simple:\r\n","          self.cnn.add(Convolution2D(32, 3, padding=constant.PADDING_SAME, input_shape=self.shape))\r\n","          self.cnn.add(Activation(constant.RELU_ACTIVATION_FUNCTION))\r\n","          self.cnn.add(Convolution2D(32, 3, 3))\r\n","          self.cnn.add(Activation(constant.RELU_ACTIVATION_FUNCTION))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2)))\r\n","          self.cnn.add(Dropout(constant.DROP_OUT_O_25))\r\n","\r\n","          self.cnn.add(Convolution2D(64, 3, padding=constant.PADDING_SAME))\r\n","          self.cnn.add(Activation(constant.RELU_ACTIVATION_FUNCTION))\r\n","          self.cnn.add(Convolution2D(64, 3, 3))\r\n","          self.cnn.add(Activation(constant.RELU_ACTIVATION_FUNCTION))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2)))\r\n","          self.cnn.add(Dropout(constant.DROP_OUT_O_25))\r\n","\r\n","          self.cnn.add(Flatten())\r\n","          self.cnn.add(Dense(constant.NUMBER_FULLY_CONNECTED))\r\n","          self.cnn.add(Activation(constant.RELU_ACTIVATION_FUNCTION))\r\n","          self.cnn.add(Dropout(constant.DROP_OUT_0_50))\r\n","          self.cnn.add(Dense(self.n_classes))\r\n","          self.cnn.add(Activation(constant.SOFTMAX_ACTIVATION_FUNCTION))\r\n","        else:\r\n","          self.cnn.add(Convolution2D(input_shape=self.shape, filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n","          self.cnn.add(Convolution2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n","          self.cnn.add(Convolution2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n","          self.cnn.add(Convolution2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n","          self.cnn.add(Convolution2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(Convolution2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\r\n","          self.cnn.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n","          self.cnn.add(Flatten())\r\n","          self.cnn.add(Dense(units=4096, activation=\"relu\"))\r\n","          self.cnn.add(Dense(units=4096, activation=\"relu\"))\r\n","          self.cnn.add(Dense(units=self.n_classes, activation=\"softmax\"))\r\n","        summary = []\r\n","        self.cnn.summary(print_fn=lambda x: summary.append(x))\r\n","        summary_filename = MODEL_PATH + \"model_summary.txt\"\r\n","        if not os.path.exists(os.path.dirname(summary_filename)):\r\n","          try:\r\n","            os.makedirs(os.path.dirname(summary_filename))\r\n","          except OSError as exc: # Guard against race condition\r\n","            if exc.errno != errno.EEXIST:\r\n","                raise\r\n","        with open(summary_filename,'a') as fh:\r\n","          fh.write('\\n'.join(summary))\r\n","        print('\\n'.join(summary))\r\n","\r\n","    def train(self, n_epochs=100, batch=32, val_split=0., stps_p_epoch = None, validation_stps = None):\r\n","        checkpoint = ModelCheckpoint(MODEL_PATH+\"models/{epoch:04d}-vgg16_1.h5\", monitor='loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\r\n","        filename = MODEL_PATH+\"training_spec.txt\"\r\n","        if not os.path.exists(os.path.dirname(filename)):\r\n","          try:\r\n","            os.makedirs(os.path.dirname(filename))\r\n","          except OSError as exc: # Guard against race condition\r\n","            if exc.errno != errno.EEXIST:\r\n","                raise\r\n","        with open(filename, 'a') as f:\r\n","              f.write(\"Epochs {} \\nBatch_size {} \\nValidation_split {} \\nLearning_rate {} \\nSteps_per_epoch {} \\nValidation_steps {}\".format(n_epochs, batch, val_split, LEARNING_RATE, stps_p_epoch, validation_stps))\r\n","\r\n","        hist = self.cnn.fit(x=self.objects,\r\n","                      y=self.labels,\r\n","                      #validation_data=(self.obj_validation, self.labels_validation),\r\n","                      batch_size=batch,\r\n","                      epochs=n_epochs,\r\n","                      validation_split=val_split,\r\n","                      validation_steps=validation_stps,\r\n","                      steps_per_epoch=stps_p_epoch,\r\n","                      shuffle=True, \r\n","                      callbacks=[checkpoint])\r\n","        \r\n","        model_training = \"\"\r\n","        for i in range(0, len(hist.history['accuracy'])):\r\n","          model_training += \"Epoch: {} ;Loss: {} ;Accuracy: {}\\n\".format(\r\n","              i, hist.history['loss'][i], hist.history['accuracy'][i]\r\n","          )\r\n","        model_results_filename = MODEL_PATH+\"model_training_result.txt\"\r\n","        with open(model_results_filename, 'w') as f:\r\n","              f.write(model_training)\r\n","        \r\n","        plt.plot(range(1,len(hist.history['accuracy'])+1), hist.history['accuracy'])\r\n","        #plt.plot(range(1,len(hist.history['val_accuracy'])+1), hist.history['val_accuracy'])\r\n","        plt.title('Model accuracy over epoch')\r\n","        plt.ylabel('Accuracy')\r\n","        plt.xlabel('Epoch')\r\n","        #plt.legend(['Accuracy', 'Validation accuracy'])\r\n","        plt.savefig(fname = MODEL_PATH + 'model_accuracy.png', dpi = 300)\r\n","\r\n","        plt.clf()\r\n","\r\n","        plt.plot(range(1,len(hist.history['loss'])+1), hist.history['loss'])\r\n","        #plt.plot(range(1,len(hist.history['val_loss'])+1), hist.history['val_loss'])\r\n","        plt.title('Model loss over epoch')\r\n","        plt.ylabel('Loss')\r\n","        plt.xlabel('Epoch')\r\n","        #plt.legend(['Loss', 'Validation loss'])\r\n","        plt.savefig(fname = MODEL_PATH + 'model_loss.png', dpi = 300)\r\n","      \r\n","    def get_model(self):\r\n","        return self.cnn\r\n","\r\n","    def predict(self, image):\r\n","        image = Common.to_float(image)\r\n","        result = self.cnn.predict(image)\r\n","        print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XPDNgAhHPnX7"},"source":["Initialize the convolutional model and set the path for saving the model and all its data"]},{"cell_type":"markdown","metadata":{"id":"k-Zsc6F09fPt"},"source":["# 2. CNN Training\n","\n","Run only one \n","- **2.1.a** for identity classification model training\n","- **2.1.b** for ethnicity classification model training\n","- **2.1.c** for gender classification model training"]},{"cell_type":"markdown","metadata":{"id":"H1bJeklK9-m3"},"source":["**2.1.a** Set dataset for identity classification model training"]},{"cell_type":"code","metadata":{"id":"MsEVllqW9-G1"},"source":["dataSet.set_identity_labels()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGdlofYl-eXp"},"source":["**2.1.b** Set dataset for ethnicity classification model training"]},{"cell_type":"code","metadata":{"id":"BLegfSqV-h1z"},"source":["dataSet.set_ethnicity_labels()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"152yAayV-khl"},"source":["**2.1.c** Set dataset for gender classification model training"]},{"cell_type":"code","metadata":{"id":"BfND4DXK-mFK"},"source":["dataSet.set_gender_labels()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQAeGugJ9xtj"},"source":["**2.2** Set the path for saving the model data and results and setup learning rate, initialize the CNN model with the dataset and flag \"Simple\".\r\n","\r\n","*   Simple = True -> A very simple CNN architecture is used\r\n","*   Simple = False -> VGG CNN architecture is used\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"_boWS9qBLcL7"},"source":["MODEL_PATH= \"model-xx-identity/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJ33obqpjD9S"},"source":["LEARNING_RATE = 0.01 \n","\n","cnn = ConvolutionalModel(dataSet)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lClyi9973eNB"},"source":["**2.3** Start training the CNN wiht the defined parameters"]},{"cell_type":"code","metadata":{"id":"H2kKn4qWhK0h"},"source":["cnn.train(n_epochs=130, val_split=0., batch = 32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDGKE-67-3XD"},"source":["# 3. CNN Evaluation\n","\n","This section is dedicated to evaluate all the models stored in the same folder. If you used the training above, models from all epochs for the same training will be stored in the same directory. Evaluatse number of correctly classifies objets for top1, top5 and top10 for each model.\n","\n","<br>\n","\n","Run only one \n","- 3.1.a to evaluate the identity classification model\n","- 3.1.b to evaluate the ethnicity classification model\n","- 3.1.c to evaluate the gender classification model"]},{"cell_type":"markdown","metadata":{"id":"2UhGGFbhBrqV"},"source":["**3.1.a** Set to evaluate identity classification models"]},{"cell_type":"code","metadata":{"id":"cBWfE4DkB3RV"},"source":["TYPE = \"identity\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JqjONoyB3dk"},"source":["**3.1.b** Set to evaluate ethnicity classification models"]},{"cell_type":"code","metadata":{"id":"NQ6XpzVUB5fQ"},"source":["TYPE = \"ethnicity\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8cKj9acB5mM"},"source":["**3.1.c** Set to evaluate gender classification models"]},{"cell_type":"code","metadata":{"id":"M0mjJ34UB82K"},"source":["TYPE = \"gender\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dSvZPmHcPreq"},"source":["**3.2 (or skip to 3.4 if test data file is already present)** Load test data\n","\n","*(requires executed code at 1.1 for the class Common)*"]},{"cell_type":"code","metadata":{"id":"WrqcTz-bYsQB"},"source":["from keras_preprocessing.image import load_img, img_to_array\n","\n","FILE_TEST_CSV = \"awe-test.csv\"\n","\n","names = []\n","images = []\n","labels = []\n","\n","labels_identites = []\n","labels_ethnicities = []\n","labels_genders = []\n","\n","for line in open(FILE_TEST_CSV):\n","    csv_row = line.split(\",\")\n","    file_name = csv_row[1]\n","    names.append(file_name)\n","    image = load_img(\"awe/\" + file_name, target_size=(IMG_WIDTH, IMG_HEIGHT))\n","    image = img_to_array(image)\n","    image = Common.reshape_from_img(image)\n","    label = int(csv_row[2]) - 1\n","    images.append(image)\n","    labels_identites.append(label)\n","    with open('awe/'+file_name.split('/')[0]+'/annotations.json') as f:\n","      data = json.load(f)\n","      ethnicity = int(data[\"ethnicity\"])-1\n","      if ethnicity is 98: # class 99 gets mapped to 8 (7 + 1)\n","        ethnicity = 7\n","      labels_ethnicities.append(ethnicity)\n","      gender = 1 # 1 means female, 0 means male\n","      if data[\"gender\"] is \"m\":\n","        gender = 0\n","      elif data[\"gender\"] is \"f\":\n","        gender = 1\n","      else:\n","        gender = 2 \n","      labels_genders.append(gender)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKRCBh0KZf5u"},"source":["**3.3 (optional)** Save loaded data into a file to just read it later"]},{"cell_type":"code","metadata":{"id":"h3dVmnl9ZLHi"},"source":["import pickle\n","f = open('test_dataset.pckl', 'wb')\n","pickle.dump([names, images, labels, labels_identites, labels_ethnicities, labels_genders], f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bYeJthNCZo8z"},"source":["**3.4 (optional) - (execute 4.2 and 4.3 if no file is present)** Load data into a variables so no processing is needed"]},{"cell_type":"code","metadata":{"id":"mO8zobbkZJ9R"},"source":["import pickle\n","f = open('test_dataset.pckl', 'rb')\n","names, images, labels, labels_identites, labels_ethnicities, labels_genders = pickle.load(f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2yA9wAOAoEs3"},"source":["**3.5 (Optional - if not already set in training)** Define the path of the models you wish to evaluate "]},{"cell_type":"code","metadata":{"id":"sp7d-DAcoEUl"},"source":["MODEL_PATH= \"model-xx-identity/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kLV4p8zZZII"},"source":["**3.6** Evaluate top1, top5 and top10 of the selected models per epoch on test data\n","\n","*(requires executed code at 1.1 for the class Common)*"]},{"cell_type":"code","metadata":{"id":"aN8-youypB56"},"source":["from tensorflow.python.keras.models import load_model\r\n","import numpy as np\r\n","import glob\r\n","import os\r\n","import matplotlib.pyplot as plt\r\n","import pickle\r\n","\r\n","if TYPE is \"ethnicity\":\r\n","  labels = labels_ethnicities\r\n","elif TYPE is \"gender\":\r\n","  labels = labels_genders\r\n","else:\r\n","  labels = labels_identites\r\n","\r\n","LOAD_FROM_FILE = False\r\n","\r\n","if LOAD_FROM_FILE:\r\n","  f = open(MODEL_PATH + 'evaluation_data.pckl', 'rb')\r\n","  epochs, total_correct, total_top5correct, total_top10correct = pickle.load(f)\r\n","  f.close()\r\n","else:\r\n","  total_correct = []\r\n","  total_incorrect = []\r\n","  total_top5correct = []\r\n","  total_top5incorrect = []\r\n","  total_top10correct = []\r\n","  total_top10incorrect = []\r\n","  epochs = []\r\n","\r\n","images = Common.reshape_transform_data(images)\r\n","try:\r\n","  for m_path in glob.glob(MODEL_PATH + \"models/*.h5\"):\r\n","    print(m_path)\r\n","    epoch = m_path.split(\"/\")[2].split(\"-\")[0]\r\n","    if int(epoch) in epochs:\r\n","      continue\r\n","    print(\"Evaluating model for epoch: \" + epoch)\r\n","    model = load_model(m_path)\r\n","    epochs.append(int(epoch))\r\n","    correct = 0\r\n","    incorrect = 0\r\n","    top5correct = 0\r\n","    top5incorrect = 0\r\n","    top10correct = 0\r\n","    top10incorrect = 0\r\n","    results_text = \"\"\r\n","    for i in range(0, len(images)):\r\n","        image = images[i]\r\n","        label = labels[i]\r\n","        prediction = model.predict(np.expand_dims(image, axis=0))\r\n","        predicted_label = np.argmax(prediction)\r\n","        if predicted_label == label:\r\n","            correct += 1\r\n","            results_text += \"Image \" + names[i] + \" correctly classified in Top1 with label \" + str(predicted_label +1) + \"\\n\"\r\n","        else:\r\n","            incorrect += 1\r\n","            results_text += \"Image \" + names[i] + \" incorrectly classified in Top1 with label \" + str(predicted_label + 1) + \" correct is \" + str(label+1) + \"\\n\"\r\n","\r\n","        sorted_prediction = prediction[0].argsort()\r\n","\r\n","        top5_labels = sorted_prediction[-5:][::-1]\r\n","        if label in top5_labels:\r\n","            top5correct += 1\r\n","            results_text += \"Image \" + names[i] + \" correctly classified in Top5 with label \" + str(predicted_label + 1) + \"\\n\"\r\n","        else:\r\n","            top5incorrect += 1\r\n","            results_text += \"Image \" + names[i] + \" incorrectly classified not in Top5 with label \" + str(\r\n","                predicted_label + 1) + \" correct is \" + str(label + 1) + \"\\n\"\r\n","            \r\n","        top10_labels = sorted_prediction[-10:][::-1]\r\n","        if label in top10_labels:\r\n","            top10correct += 1\r\n","            results_text += \"Image \" + names[i] + \" correctly classified in Top10 with label \" + str(predicted_label + 1) + \"\\n\"\r\n","        else:\r\n","            top10incorrect += 1\r\n","            results_text += \"Image \" + names[i] + \" incorrectly classified not in Top10 with label \" + str(\r\n","                predicted_label + 1) + \" correct is \" + str(label + 1) + \"\\n\"\r\n","        \r\n","    top1_results = \"Rank 1 - Correctly classified: {} - Incorrectly classified: {}\".format(correct, incorrect)\r\n","    top5_results = \"Rank 5 - Correctly classified: {} - Incorrectly classified: {}\".format(top5correct, top5incorrect)\r\n","    top10_results = \"Rank 10 - Correctly classified: {} - Incorrectly classified: {}\".format(top10correct, top10incorrect)\r\n","\r\n","    filename = MODEL_PATH+\"results/\"+epoch+\"-test_results.txt\"\r\n","    if not os.path.exists(os.path.dirname(filename)):\r\n","      try:\r\n","        os.makedirs(os.path.dirname(filename))\r\n","      except OSError as exc: # Guard against race condition\r\n","        if exc.errno != errno.EEXIST:\r\n","            raise\r\n","    with open(filename, 'a') as f:\r\n","      data =  top1_results + \"\\n\" + top5_results + \"\\n\" +  top10_results + \"\\n\\n\" + results_text\r\n","      f.write(data)\r\n","\r\n","    print(top1_results)\r\n","    print(top5_results)\r\n","    print(top10_results)\r\n","    print(\"\\n\")\r\n","    total_correct.append(correct)\r\n","    total_top5correct.append(top5correct)\r\n","    total_top10correct.append(top10correct)\r\n","except Exception as e:\r\n","  print(e)\r\n","  f = open(MODEL_PATH + 'evaluation_data.pckl', 'wb')\r\n","  pickle.dump([epochs, total_correct, total_top5correct, total_top10correct], f)\r\n","  f.close()\r\n","\r\n","\r\n","\r\n","total_correct_percent = numpy.array(total_correct)/250\r\n","total_top5correct_percent = numpy.array(total_top5correct)/250\r\n","total_top10correct_percent = numpy.array(total_top10correct)/250\r\n","\r\n","best_epochs_filename = MODEL_PATH + \"best_epoch.txt\"\r\n","with open(best_epochs_filename, 'a') as f:\r\n","  best_epoch_top1 = total_correct.index(max(total_correct))\r\n","  best_epoch_top5 = total_top5correct.index(max(total_top5correct))\r\n","  best_epoch_top10 = total_top10correct.index(max(total_top10correct))\r\n","  data = \"Rank 1 - best epoch: {} with top1_correct: {} top5_correct: {} top10_correct: {}\\n\".format(epochs[best_epoch_top1], total_correct[best_epoch_top1], total_top5correct[best_epoch_top1], total_top10correct[best_epoch_top1])\r\n","  data += \"Rank 5 - best epoch: {} with top1_correct: {} top5_correct: {} top10_correct: {}\\n\".format(epochs[best_epoch_top5], total_correct[best_epoch_top5], total_top5correct[best_epoch_top5], total_top10correct[best_epoch_top5])\r\n","  data += \"Rank 10 - best epoch: {} with top1_correct: {} top5_correct: {} top10_correct: {}\".format(epochs[best_epoch_top10], total_correct[best_epoch_top10], total_top5correct[best_epoch_top10], total_top10correct[best_epoch_top10])\r\n","  print(\"\\n\\nOveral best:\\n\" + data)\r\n","  f.write(data)\r\n","\r\n","plt.plot(epochs, total_correct_percent)\r\n","plt.plot(epochs, total_top5correct_percent)\r\n","plt.plot(epochs, total_top10correct_percent)\r\n","plt.title('Correctly classified images over epoch')\r\n","plt.ylabel('Recognition rate')\r\n","plt.xlabel('Epoch')\r\n","plt.legend(['Rank 1', 'Rank 5', 'Rank 10'])\r\n","plt.savefig(fname = MODEL_PATH + 'model_validation.png', dpi = 300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gpmDIeIH5jfl"},"source":["# 4. Single model evaluation\n","\n","This section is dedicated to evaluate a single manually picked model and get number of correctly classifies objets for top1, top5 and top10."]},{"cell_type":"markdown","metadata":{"id":"49PYmXC66CTZ"},"source":["**4.1 (or skip to 4.2 if test data file is already present)** Load test data"]},{"cell_type":"code","metadata":{"id":"B54lgzMg6C12"},"source":["from keras_preprocessing.image import load_img, img_to_array\n","import json\n","\n","FILE_TEST_CSV = \"awe-test.csv\"\n","\n","IMG_WIDTH=224\n","IMG_HEIGHT=224\n","\n","names = []\n","images = []\n","labels = []\n","\n","labels_identites = []\n","labels_ethnicities = []\n","labels_genders = []\n","\n","for line in open(FILE_TEST_CSV):\n","    csv_row = line.split(\",\")\n","    file_name = csv_row[1]\n","    names.append(file_name)\n","    image = load_img(\"awe/\" + file_name, target_size=(IMG_WIDTH, IMG_HEIGHT))\n","    image = img_to_array(image)\n","    image = image.reshape((IMG_WIDTH, IMG_HEIGHT, 3))\n","    label = int(csv_row[2]) - 1\n","    images.append(image)\n","    labels_identites.append(label)\n","    with open('awe/'+file_name.split('/')[0]+'/annotations.json') as f:\n","      data = json.load(f)\n","      ethnicity = int(data[\"ethnicity\"])-1\n","      if ethnicity is 98:\n","        ethnicity = 7\n","      labels_ethnicities.append(ethnicity)\n","      gender = 1 # 1 means female, 0 means male\n","      if data[\"gender\"] is \"m\":\n","        gender = 0\n","      elif data[\"gender\"] is \"f\":\n","        gender = 1\n","      else:\n","        gender = 2 \n","      labels_genders.append(gender)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ge3CbCNw6DBW"},"source":["**4.2 (optional)** Load test data from file"]},{"cell_type":"code","metadata":{"id":"U_Lhemhd6DLR"},"source":["import pickle\n","f = open('test_dataset.pckl', 'rb')\n","names, images, labels, labels_identites, labels_ethnicities, labels_genders = pickle.load(f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y486iX8Pr-qn"},"source":["**4.3** Define method for evaluating the selected model for selected type"]},{"cell_type":"code","metadata":{"id":"aWpiOD3W5koG"},"source":["from tensorflow.python.keras.models import load_model\n","from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import numpy as np\n","import glob\n","import os\n","\n","def calculate_rank(model_path, type):\n","  n_classes = 0\n","  if type is \"ethnicity\":\n","    labels = labels_ethnicities\n","    n_classes = 7\n","  elif type is \"gender\":\n","    labels = labels_genders\n","    n_classes = 2\n","  else:\n","    labels = labels_identites\n","    n_classes = 100\n","  model = load_model(model_path)\n","  ranks = [0] * n_classes\n","  for i in range(0, len(images)):\n","      image = images[i]\n","      label = labels[i]\n","      prediction = model.predict(np.expand_dims(image, axis=0))\n","      sorted_prediction = prediction[0].argsort()\n","      for r in range(1, n_classes+1):\n","        if label in sorted_prediction[-r:][::-1]:\n","          ranks[r-1] += 1\n","\n","\n","  #print(\"Top 1 {}\".format(ranks[0]))\n","  #print(\"Top 5 {}\".format(ranks[4]))\n","  #print(\"Top 5 {}\".format(ranks[9]))\n","  \n","  ranks_rate = np.array(ranks)/250 \n","\n","  filename = \"results/\"+model_path+\"/test_results_ranks.txt\"\n","  if not os.path.exists(os.path.dirname(filename)):\n","    try:\n","      os.makedirs(os.path.dirname(filename))\n","    except OSError as exc: # Guard against race condition\n","      if exc.errno != errno.EEXIST:\n","          raise\n","  with open(filename, 'a') as f:\n","    ranks_array = np.char.mod('%f', ranks_rate)\n","    data = \"\\n\".join(ranks_array)\n","    f.write(data)\n","  return ranks_rate\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yu1i-Kpm_-pL"},"source":["**4.4** Evaluate the selected model on test data and obtain data for CMC curve \r\n","- 4.4.a to evaluate the identity classification model\r\n","- 4.4.b to evaluate the ethnicity classification model\r\n","- 4.4.c to evaluate the gender classification model"]},{"cell_type":"markdown","metadata":{"id":"hERHIo1Jry4_"},"source":["**4.4.a** Set to evaluate identity classification models"]},{"cell_type":"code","metadata":{"id":"UbM6L-IZKBur"},"source":["vgg_full = calculate_rank(\"model-18-identity/models/0087-vgg16_1.h5\", \"identity\")\n","f = open('vgg_full_identity.pckl', 'wb')\n","pickle.dump(vgg_full, f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ugEUq7zKDI0"},"source":["vgg_07 = calculate_rank(\"model-16-identity/models/0055-vgg16_1.h5\", \"identity\")\n","f = open('vgg_07_identity.pckl', 'wb')\n","pickle.dump(vgg_07, f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kfmwa4-KFgr"},"source":["simple_full = calculate_rank(\"model-simple-identity/models/0057-simple_1.h5\", \"identity\")\n","f = open('simple_full_identity.pckl', 'wb')\n","pickle.dump(simple_full, f)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8iBgc4UDSJ_","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1610304717226,"user_tz":-60,"elapsed":1223,"user":{"displayName":"David Valic","photoUrl":"","userId":"13603712033004126286"}},"outputId":"ed876bb8-bc86-4779-e4a3-9355117aa364"},"source":["f = open('vgg_full_identity.pckl', 'rb')\n","vgg_full = pickle.load(f)\n","f.close()\n","f = open('vgg_07_identity.pckl', 'rb')\n","vgg_07 = pickle.load(f)\n","f.close()\n","f = open('simple_full_identity.pckl', 'rb')\n","simple_full = pickle.load(f)\n","f.close()\n","\n","\n","plt.plot(range(1, 100 +1 ), vgg_full)\n","plt.plot(range(1, 100 +1 ), vgg_07)\n","plt.plot(range(1, 100 +1 ), simple_full)\n","plt.title('CMC for identity ')\n","plt.ylabel('Recognition rate')\n","plt.xlabel('Rank')\n","plt.legend(['VGG full training', 'VGG 0.7 training', 'Simple full training'])\n","plt.savefig(fname = \"results/model_rank_identity.png\", dpi = 300)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"3naG1MAmr0zy"},"source":["**4.4.b** Set to evaluate ethnicity classification models"]},{"cell_type":"code","metadata":{"id":"2VzCvpXaCa4_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610305038285,"user_tz":-60,"elapsed":34049,"user":{"displayName":"David Valic","photoUrl":"","userId":"13603712033004126286"}},"outputId":"e52f37e4-f996-44ea-fcf0-412931fb3420"},"source":["vgg_full = calculate_rank(\"model-18-ethnicity/models/0026-vgg16_1.h5\", \"ethnicity\")\n","vgg_full_arr = np.char.mod('%f', vgg_full)\n","vgg_07 = calculate_rank(\"model-16-ethnicity/models/0003-vgg16_1.h5\", \"ethnicity\")\n","vgg_07_arr = np.char.mod('%f', vgg_07)\n","simple_full = calculate_rank(\"model-simple-ethnicity/models/0030-simple_1.h5\", \"ethnicity\")\n","simple_full_arr = np.char.mod('%f', simple_full)\n","\n","print(\"VGG Full Ethnicity Rank:    \" + \" | \".join(vgg_full_arr))\n","print(\"VGG 0.7 Ethnicity Rank:     \" + \" | \".join(vgg_07_arr))\n","print(\"Simple Full Ethnicity Rank: \" + \" | \".join(simple_full_arr))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top 1 155\n","Top 1 153\n","Top 1 159\n","VGG Full Ethnicity Rank:    0.620000 | 0.812000 | 0.912000 | 0.936000 | 0.956000 | 0.988000 | 1.000000\n","VGG 0.7 Ethnicity Rank:     0.612000 | 0.800000 | 0.916000 | 0.948000 | 0.976000 | 0.988000 | 1.000000\n","Simple Full Ethnicity Rank: 0.636000 | 0.840000 | 0.912000 | 0.940000 | 0.956000 | 0.984000 | 1.000000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UGzE79Sir3Em"},"source":["**4.4.c** Set to evaluate gender classification models"]},{"cell_type":"code","metadata":{"id":"6OCEoGBmCkeF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610305135961,"user_tz":-60,"elapsed":71584,"user":{"displayName":"David Valic","photoUrl":"","userId":"13603712033004126286"}},"outputId":"a170c237-0328-483b-f13b-f16d4a7b9a3b"},"source":["vgg_full = calculate_rank(\"model-18-gender/models/0001-vgg16_1.h5\", \"gender\")\n","vgg_full_arr = np.char.mod('%f', vgg_full)\n","vgg_07 = calculate_rank(\"model-16-gender/models/0001-vgg16_1.h5\", \"gender\")\n","vgg_07_arr = np.char.mod('%f', vgg_07)\n","simple_full = calculate_rank(\"model-simple-gender/models/0001-simple_1.h5\", \"gender\")\n","simple_full_arr = np.char.mod('%f', simple_full)\n","\n","print(\"VGG Full Gender Rank:    \" + \" | \".join(vgg_full_arr))\n","print(\"VGG 0.7 Gender Rank:     \" + \" | \".join(vgg_07_arr))\n","print(\"Simple Full Gender Rank: \" + \" | \".join(simple_full_arr))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top 1 229\n","Top 1 229\n","Top 1 229\n","VGG Full Gender Rank:    0.916000 | 1.000000\n","VGG 0.7 Gender Rank:     0.916000 | 1.000000\n","Simple Full Gender Rank: 0.916000 | 1.000000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y6v0WNEHehav"},"source":["# ResNet - WIP"]},{"cell_type":"markdown","metadata":{"id":"GL07HKL-jom1"},"source":["Resnet example - doesnt work yet"]},{"cell_type":"code","metadata":{"id":"UCt2Om6bROB4"},"source":["import numpy\n","from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.python.keras.layers import Convolution2D, Activation, MaxPooling2D, Dropout, Flatten, Dense, GlobalAveragePooling2D\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","\n","\n","checkpoint = ModelCheckpoint(\"100-resnet_1.h5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n","\n","\n","class ConvolutionalResModel():\n","\n","    def __init__(self, dataSet=None):\n","        if dataSet is None:\n","            raise Exception(\"DataSet is required in this model\")\n","        self.shape = numpy.array([IMG_WIDTH, IMG_HEIGHT, 3])\n","        if dataSet is not None:\n","            self.objects = dataSet.objects\n","            self.labels = dataSet.labels\n","            #self.obj_validation = dataSet.obj_validation\n","            #self.labels_validation = dataSet.labels_validation\n","            self.number_labels = dataSet.number_labels\n","            self.n_classes = dataSet.n_classes\n","        self.init_model()\n","        self.cnn.compile(loss='categorical_crossentropy',\n","                         optimizer=Common.get_sgd_optimizer(),\n","                         metrics=['accuracy'])\n","\n","    def init_model(self):\n","        base_model = ResNet50(weights= None, include_top=False, input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n","        base_model = GlobalAveragePooling2D()(base_model)\n","        base_model = Dropout(0.7)(base_model)\n","        predictions = Dense(self.num_classes, activation= 'softmax')(base_model)\n","        self.cnn = Model(inputs = base_model.input, outputs = predictions)\n","        self.cnn.summary()\n","\n","    def train(self, n_epochs=100, batch=32):\n","        self.cnn.fit(self.objects, self.labels,\n","                       batch_size=batch,\n","                       epochs=n_epochs,\n","                     validation_split=0.05,\n","                     steps_per_epoch=30,\n","                     shuffle=True, \n","                     callbacks=[checkpoint])\n","\n","    def get_model(self):\n","        return self.cnn\n","\n","    def predict(self, image):\n","        image = Common.to_float(image)\n","        result = self.cnn.predict(image)\n","        print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGclxjXSc6w3"},"source":["cnn = ConvolutionalResModel(dataSet)\n","cnn.train(n_epochs=100)"],"execution_count":null,"outputs":[]}]}